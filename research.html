<html class="gr__">
  <head>
    <meta charset="utf-8">
    <title>research</title>
    <link rel="stylesheet" type="text/css" href="styles/notebook_style.css">
    <style>
        header {
          text-align: center;
          margin: 10px;
        } 
        nav {
          width: 100%;
          display: inline-block;
          text-align: center;
          padding: 10px;
          list-style: none;
          background-color: #ffffff; 
        }
        html {
          font-family: "Helvetica Neue", Sans-Serif; /*Helvetica*/
          font-weight: 15;
          font-size: 16px;
        }
        nav li {
          display: inline-block;
          padding: 0 100px;
          text-align: center; 
          color: #ffffff;
        }
    
        a:link, a:visited {
          color: gray;
          text-decoration: none;
        }
    
        a:hover, a:active {
          color: black;
        }
   
        img:hover {
          opacity:0.8;
        }
   
        #logo_pic:hover {
          opacity:0.4;
        }
    </style>
  </head>

  <body data-gr-c-s-loaded="true">
    <header id="top">
      <img src="images/logo.png" id="logo_pic" alt="logo" style="width: 200px;">
    </header>
    
    <nav>
      <a href="index.html">Home</a>
      <a href="research.html">Research</a>
      <a href="https://github.com/chaxor.html">Code</a>
      <a href="https://scholar.google.com/citations?user=qjRD8Z8AAAAJ&amp;hl=en&amp;authuser=1">Publications</a>
      <a href="courses.html">Courses</a>
      <a href="design.html">Design</a>                                                       
    </nav>

    <br>

    <div tabindex="-1" id="notebook" class="border-box-sizing">

      <div class="container" id="notebook-container">
        <div class="cell border-box-sizing text_cell rendered">
          <div class="prompt input_prompt"></div>
          <div class="inner_cell">
            <div class="text_cell_render border-box-sizing rendered_html">


              <h1 id="Current-Research-Topics">Current Research Topics<a class="anchor-link" href="#Current-Research-Topics">¶</a></h1>

              <p>My current research is in developing and utilizing the tools from an emerging field referred to as the "science of Science" in order to:</p>
              <ul>
                <li>Understand and find artificial neural network architectures for the neural processes by which researchers make decisions on research topics</li>
                <li>Develop automated processes for generating hypotheses and scoring them via a test of likelihood of a valid discovery</li>
                <li>Create a drug discovery system based off of conditional responses to drugs (tissue-specific, age-specific, or drug synergy responses)</li>
                <li>Use the discovery system to find new, potentially synergistic therapies for glioblastoma</li>
              </ul>


              <h2 id="Literature-based-Discovery">Literature-based Discovery<a class="anchor-link" href="#Literature-based-Discovery">¶</a></h2>

              <p>Literature based discovery (LBD) is a well established field with a history closely linked to network analysis.  LBD is typically performed by constructing a large database of textual information (such as abstracts from MEDLINE), extracting entities or concepts of interest from each unit of text, and creating a network or graph of the entities, with edges between entities that <i>co-occur</i> within the same document.  In this manner, a large network of entities and their relations is created as a type of 'condensation' or 'compression' of the knowledge within the text.  Although this method may seem crude and lacking in contextual detail for each entity and it's relation to other entities, this method has provided many successful predictions of the trajectory of science over the years.<sup>1,2</sup></p>
              <p>
                However, progress within the field is quite sparse, as the differences between recently published articles<sup>2</sup> and similar research over a decade ago<sup>1</sup> are minimal, if existent at all.  Therefore, I am interested in pushing the boundaries of this field further, with the incorporation of multiple relationship type, transcriptomic, epigenetic, and drug perturbation information.
              </p>


              <h3 id="Natural-Language-Processing">Natural Language Processing<a class="anchor-link" href="#Natural-Language-Processing">¶</a></h3>

              <p>Natural Language Processing is a necessary tool for creating literature-based discovery systems.  In order to improve upon previously implemented LBD systems, information must be extracted regarding the relationships between the entities within the text.  One example of the types of available relationships would be inhibition between a drug and an enzyme (such as Rapamycin <i>inhibits</i> mTOR).  There are numerous methodologies and tools used to extract these types of relationships developed over the years within the field of NLP.  However, not all of them are highly performant.</p>


              <h3 id="Neural-Networks">Neural Networks<a class="anchor-link" href="#Neural-Networks">¶</a></h3>

              <p>Recently, neural networks have exploded in popularity due to the increase in available computational power via GPUs and TPUs.  Additionally, these algorithms have, in recent years, allowed huge performance gains for language tasks due to thier excellent ability to model non-linear data.  
                <br>
                Several different types of neural architectures have been implemented with the purpose of extracting entities and relationships from text.  Early work in NLP showed that simple multi-layer perceptrons can achieve performances similar to that of support vector machines in named entity recognition.<sup>3</sup> Additionally, some convolutional network architectures have seen success in similar textual tagging tasks, wherein only a relatively small and local amount of contextual information is required to achieve a decent performance. While these simpler neural systems have seen some success comparable to support vector machines or CRFs, recent works have seen a great increase in performance metrics via the use of recurrent neural networks (RNNs) and more specifically long short-term memory recurrent neural networks (LSTMs).  
    However, the most recent and powerful  neural method, neural attention has been made extremely influential to the field.  Some of the earliest recognition of the utility of attention mechanisms came from combinations of LSTMs and attention.<sup>4</sup>  Nonetheless, the acknowledgement that the attention mechanism itself may be useful on it’s own came from a major paper in the field, with a fitting title Attention is all you need.<sup>5</sup> The key finding from this article which evolved into major advancements for many fields is the Transformer architecture.  Specifically within the field of NLP, the Transformer architecture saw its first huge success in bidirectional encoder representations from transformers (BERT).<sup>6</sup>  BERT is a masked language model, pre-trained to perform tasks such as predicting missing words and next sentence similarity. This type of pretraining step allowed the fine-tuned berT model to outperform state-of-the-art models in several NLP areas, such as NER, relationship extraction, question answering, and textual entailment.  Several other models have followed which expand upon BERT’s original capabilities, such as transformer-XL<sup>7</sup>, ALBERT<sup>8</sup>, DistilBERT<sup>9</sup>, and RoBERTa.<sup>10</sup></p>


              <h3 id="Algebraic-Topology">Algebraic Topology<a class="anchor-link" href="#Algebraic-Topology">¶</a></h3>

              <p>Algebraic Topology is a field which has great potential for answering questions in neural processes, both artificial and biological in nature.<sup>11,12</sup>  Addiitionally, a tool within the field of Algebraic Topology, known as Persistent Homology, allows the study of topological holes within datasets, which are typically reffered to as 'knowledge gaps' within the context of LBD co-occurence networks.
              <br>

              (Expanding upon this section soon.)
              </p>


              <h2 id="Drug-Discovery">Drug Discovery<a class="anchor-link" href="#Drug-Discovery">¶</a></h2>

              <p>While LBD techniques have continued to receive attention even in today's academic environment, LBD via co-occurrence has been thoroughly investigated for decades. Therefore, it is likely that, in order to tackle questions regarding disease in today's academic environment, more is required than standard literature-based discovery techniques.</p>
              <p>In order to address this disjoint between text-only sources and biological assay driven discoveries, we seek to develop both solutions, and join them together to search the solution space with more complete information.</p>


              <h3 id="Tensor-Fields-for-Signature-Searches">Tensor Fields for Signature Searches<a class="anchor-link" href="#Tensor-Fields-for-Signature-Searches">¶</a></h3>

              <p>The realization of drug-induced differential gene signatures as a useful tool for drug discovery is pervasive throughout our biological understanding.</p>
              <p>However, few groups have noticed the alteration of drug-induced differential gene signatures within different conditions, such as tissue, age, or sex.</p>
              <p>One manner to address this issue is to simply find data for each tissue or condition and evaluate the differential gene list between drug-treated and control.  However, while this approach (if conditions are considered at all) is the standard, much more information can be gained from considering generalities between conditions and drugs.</p>
              <p>In order to do this, we look at the entire transcriptome available for all conditions and drug treatments available via public information (most notably, the LINCS dataset).  By using <em>tensor fields</em>, we can look at smoothed transitions of drug-induced differential gene vectors within a conditional vector space.</p>
              <p>This analysis allows to gain an understanding of the attractors  within the vector field of biologically allowable states.</p>


              <h2 id="Glioblastoma">Glioblastoma<a class="anchor-link" href="#Glioblastoma">¶</a></h2>

              <p>Glioblastoma is a common nervous system tumor with an abysmal survival rate, despite continual treatment efforts. Bioinformatics systems Rephetio<sup>13</sup> and Iridescent<sup>1</sup> provide drug repurposing predictions, and can suggest small-molecule therapies for glioblastoma. Similarly, suggestions for transmembrance protein targets have been suggested via Iridescent, which have yeilded viable immunotherapies for the disease.
                <br> Unfortunately, however, the survival rate for this disease still maintains one of the hardest to combat, as several technical challenges specific to brain caner hinder progress, such as the blood-brain barrier.
              Therefore, our automated discovery system which incorporates literature, transcriptomic, and epigenetic data will be challenged to find novel insights and therapies for this disease.
              </p>


              <h1 id="Previous-Research">Previous Research<a class="anchor-link" href="#Previous-Research">¶</a></h1>

              <p>My previous research was heavily focused upon nanomaterials, optoelectronics, photophysics, and heterogeneous catalysis.</p>
              <p>I previously worked for SouthWest Nanotechnologies, a company which was founded by a highly respected heterogeneous catalysis professor, Daniel E Resasco.  The company was founded upon the improvements to SWCNT production allowed by the heterogeneous catalyst of Cobalt and Molybdenum (CoMo) on silica or Alumina based supports.  My research at the company started with investigation into the characterization of SWCNT properties such as the electronic and optical properties, diameter, and length; however, it soon increased in scope to include separations and heterogeneous catalysis synthesis and optimization.</p>
              <p>I additionally began working towards a Master's degree while also maintaining my position as a Lead Research and Development Engineer at SWeNT. The topic of my Master's thesis involved my expertise on SWCNT properties, but relied more heavily upon heterogeneous catalysis towards biofuel production (rather than SWCNT production) and defect or functionalization of SWCNTs (rather than optical properties of pristine SWCNTs).</p>

<!--
              <h2 id="Janus-Nanoparticles-as-heterogeneous-Catalytic-Support-in-Multiphase-Reactors-for-Biofuel-Production">Janus Nanoparticles as heterogeneous Catalytic Support in Multiphase Reactors for Biofuel Production<a class="anchor-link" href="#Janus-Nanoparticles-as-Heterogeneous-Catalytic-Support-in-Multiphase-Reactors-for-Biofuel-Production">¶</a></h2>

              <p>My Master's research</p>

              <h2 id="Single-Walled-Carbon-Nanotube-(SWCNT)-Characterization">Single-Walled Carbon Nanotube (SWCNT) Characterization<a class="anchor-link" href="#Single-Walled-Carbon-Nanotube-(SWCNT)-Characterization">¶</a></h2>

              <p>insert intro from paper here</p>

              <h3 id="SWCNT-Optical-Physics">SWCNT Optical Physics<a class="anchor-link" href="#SWCNT-Optical-Physics">¶</a></h3>

              <p>insert some stuff here</p>

              <h2 id="SWCNT-Separations">SWCNT Separations<a class="anchor-link" href="#SWCNT-Separations">¶</a></h2>

              <p>more here insert</p>

              <h3 id="Adsorption-Chromatography">Adsorption Chromatography<a class="anchor-link" href="#Adsorption-Chromatography">¶</a></h3>

              <p>insert description here</p>

              <h3 id="Aqueous-Two-Phase-Extraction">Aqueous Two Phase Extraction<a class="anchor-link" href="#Aqueous-Two-Phase-Extraction">¶</a></h3>

              <p>Aqueous two phase insert here</p>

-->

              <h2 id="References:">References:<a class="anchor-link" href="#References:">¶</a></h2>
              <ol>
                <li>Wren, Jonathan D., Raffi Bekeredjian, Jelena A. Stewart, Ralph V. Shohet, and Harold R. Garner. "Knowledge discovery by automated identification and ranking of implicit relationships." Bioinformatics 20, no. 3 (2004): 389-398.
                </li>
                
                <li>Pyysalo, Sampo, Simon Baker, Imran Ali, Stefan Haselwimmer, Tejas Shah, Andrew Young, Yufan Guo et al. "LION LBD: a literature-based discovery system for cancer biology." Bioinformatics 35, no. 9 (2019): 1553-1561.
                </li>
                
                <li>Szarvas, György, Richárd Farkas, László Felföldi, András Kocsor, and János Csirik. "A highly accurate Named Entity corpus for Hungarian." In LREC, pp. 1957-1960. 2006.</li>
                
                <li>Luo, Ling, Zhihao Yang, Pei Yang, Yin Zhang, Lei Wang, Hongfei Lin, and Jian Wang. "An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition." Bioinformatics 34, no. 8 (2018): 1381-1388.</li>
                
                <li>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. "Attention is all you need." In Advances in neural information processing systems, pp. 5998-6008. 2017.</li>
                
                <li>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).</li>
                
                <li>Dai, Zihang, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. "Transformer-xl: Attentive language models beyond a fixed-length context." arXiv preprint arXiv:1901.02860 (2019).</li>
                <li>Lan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. "Albert: A lite bert for self-supervised learning of language representations." arXiv preprint arXiv:1909.11942 (2019).</li>
                <li>Sanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter." arXiv preprint arXiv:1910.01108 (2019).</li>
                <li>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. "Roberta: A robustly optimized bert pretraining approach." arXiv preprint arXiv:1907.11692 (2019).</li>
                
                <li>
                  Rieck, Bastian, Matteo Togninalli, Christian Bock, Michael Moor, Max Horn, Thomas Gumbsch, and Karsten Borgwardt. "Neural persistence: A complexity measure for deep neural networks using algebraic topology." arXiv preprint arXiv:1812.09764 (2018).
                </li>
                
                <li>
                  Sizemore, Ann, Chad Giusti, Richard F. Betzel, and Danielle S. Bassett. "Closures and cavities in the human connectome." arXiv preprint arXiv:1608.03520 (2016).
                </li>
                <li>Himmelstein, Daniel Scott, Antoine Lizee, Christine Hessler, Leo Brueggeman, Sabrina L. Chen, Dexter Hadley, Ari Green, Pouya Khankhanian, and Sergio E. Baranzini. "Systematic integration of biomedical knowledge prioritizes drugs for repurposing." Elife 6 (2017): e26726.
                </li>
              </ol>

            </div>
          </div>
        </div>
      
      </div>
    </div>
  </body>
</html>